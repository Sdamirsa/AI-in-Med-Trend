{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "user_query='\"Artificial Intelligence\"[Mesh]'\n",
    "user_start_date=\"2000/01/01\"\n",
    "user_end_date=\"2025/03/01\"\n",
    "user_unique_experiment_name_for_files = \"AI-in-Med-2025\"\n",
    "\n",
    "\n",
    "# In case you wanted to change the CACHE_DIRECTORY to a different location the defult is \"pubmed_data\"\n",
    "# import os\n",
    "# os.environ[\"CACHE_DIRECTORY\"] = \"some/folder/you/like\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If CACHE_DIRECTORY is not set, use a default path\n",
    "if \"CACHE_DIRECTORY\" not in os.environ:\n",
    "    os.environ[\"CACHE_DIRECTORY\"] = os.path.join(os.getcwd(), \"pubmed_data\")\n",
    "    print(f\"CACHE_DIRECTORY was not set, using default path {os.environ['CACHE_DIRECTORY']}\")\n",
    "elif not os.path.isabs(os.environ[\"CACHE_DIRECTORY\"]):\n",
    "    # If it's a relative path, make it absolute\n",
    "    os.environ[\"CACHE_DIRECTORY\"] = os.path.join(os.getcwd(), os.environ[\"CACHE_DIRECTORY\"])\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(os.environ[\"CACHE_DIRECTORY\"], exist_ok=True)\n",
    "\n",
    "# Create paths using os.path.join for better compatibility across operating systems\n",
    "S2_folder_path = os.path.join(os.environ[\"CACHE_DIRECTORY\"], \"S2_output\")\n",
    "S3_folder_path = os.path.join(os.environ[\"CACHE_DIRECTORY\"], \"S3_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Retriving articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import S1_DownloadPubmed_main\n",
    "import os\n",
    "\n",
    "# S1_DownloadPubmed_main(query=user_query, start_date=user_start_date, end_date=user_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Cleaning XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import create_and_copy_folder\n",
    "import os\n",
    "\n",
    "# Create S2 folder by copying from the original experiment folder\n",
    "create_and_copy_folder(source_name=os.environ[\"CACHE_DIRECTORY\"], destination_folder=S2_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import S2_Cleaner_processor_main\n",
    "\n",
    "\n",
    "S2_Cleaner_processor_main(data_dir=S2_folder_path, combine_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import S2_prepare_and_label_main\n",
    "import os\n",
    "\n",
    "S2_prepare_and_label_main(\n",
    "    folder_path= S2_folder_path,\n",
    "    filter_startstring=\"cleaned_pubmed\",\n",
    "    add_string_at_beginning=\"\"  # empty => overwrite\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraS2: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "test_S2_folder_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\AI-in-Med-Trend\\pubmed_data_test\"\n",
    "\n",
    "######################\n",
    "####   CPU or MAC ####\n",
    "######################\n",
    "# from Code import extraS2_Embedding_processor_main\n",
    "\n",
    "# extraS2_Embedding_processor_main(folder_path = test_S2_folder_path,\n",
    "#                                  filter_startstring=\"cleaned_pubmed\",save_embedding_path=os.getenv(\"Save_FAISS_Embedding_Path\"), batch_size=50, save_format=\"faiss\",\n",
    "#                                  model_name=os.getenv(\"HF_ST_model_for_clustering\")\n",
    "#                                  )\n",
    "\n",
    "#########################\n",
    "#####   GPU & cuda ######\n",
    "#########################\n",
    "#In case you have GPU and want to run it with cude, follow instruction on Code/requirements_venv_torch.txt to create venv_torch and install torch with GPU support.\n",
    "\n",
    "from Code import extraS2_Embedding_processor_subprocess_main\n",
    "\n",
    "extraS2_Embedding_processor_subprocess_main(\n",
    "    folder_path = test_S2_folder_path,\n",
    "    filter_startstring=\"cleaned_pubmed\",\n",
    "    save_embedding_path=os.getenv(\"Save_FAISS_Embedding_Path\"),\n",
    "    batch_size=50,\n",
    "    save_format=\"faiss\",\n",
    "    model_name=os.getenv(\"HF_ST_model_for_clustering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "####   CPU or MAC ####\n",
    "######################\n",
    "from Code import extraS2_Embedding_processor_main\n",
    "\n",
    "extraS2_Embedding_processor_main(folder_path = S2_folder_path, \n",
    "                                 filter_startstring=\"cleaned_pubmed\",save_embedding_path=os.getenv(\"Save_FAISS_Embedding_Path\"), batch_size=50, save_format=\"faiss\")\n",
    "\n",
    "#########################\n",
    "#####   GPU & cuda ######\n",
    "#########################\n",
    "# In case you have GPU and want to run it with cude, follow instruction on Code/requirements_venv_torch.txt to create venv_torch and install torch with GPU support.\n",
    "\n",
    "# from Code import extraS2_Embedding_processor_subprocess_main\n",
    "\n",
    "# extraS2_Embedding_processor_subprocess_main(\n",
    "#     folder_path = S2_folder_path,\n",
    "#     filter_startstring=\"cleaned_pubmed\",\n",
    "#     save_embedding_path=os.getenv(\"Save_FAISS_Embedding_Path\"),\n",
    "#     batch_size=50,\n",
    "#     save_format=\"faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for visualization of the raw embedding (before DimenReduction using UMAP).\n",
    "\n",
    "from Code import extraS2_DimenReduction_viz_main\n",
    "import os\n",
    "\n",
    "\n",
    "viz_mapper, viz_embeddings = extraS2_DimenReduction_viz_main(os.getenv(\"Save_FAISS_Embedding_Path\"), label_for_figure=\"beforeUMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraS2: DimenReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import extraS2_DimenReduction_main\n",
    "import numpy as np\n",
    "\n",
    "reduced_embeddings= extraS2_DimenReduction_main(faiss_input_path=os.getenv(\"Save_FAISS_Embedding_Path\"),\n",
    "                            faiss_output_path=os.getenv(\"Save_FAISS_DimenReduction_Path\"),\n",
    "                            umap_metric= os.getenv(\"UMAP_METRIC\"),\n",
    "                            umap_min_dist= os.getenv(\"UMAP_MIN_DIST\"),\n",
    "                            umap_n_components= os.getenv(\"UMAP_N_COMPONENTS\"),\n",
    "                            umap_n_neighbors= os.getenv(\"UMAP_N_NEIGHBORS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for visualization of the reduced embeddings using UMAP.\n",
    "from Code import extraS2_DimenReduction_viz_main\n",
    "import os\n",
    "\n",
    "\n",
    "viz_mapper, viz_embeddings = extraS2_DimenReduction_viz_main(os.getenv(\"Save_FAISS_DimenReduction_Path\"), label_for_figure=\"UMAP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraS2: Clustering & Cluster Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Code import extraS2_Clustering_main\n",
    "\n",
    "cluster_labels = extraS2_Clustering_main(\n",
    "    faiss_input_path=os.getenv(\"Save_FAISS_DimenReduction_Path\"),\n",
    "    cluster_save_path=os.getenv(\"Save_Clustering_Path\"),\n",
    "    min_cluster_size=50,\n",
    "    min_samples=25,\n",
    "    cluster_selection_epsilon=0.4,\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    metric='euclidean'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import extraS2_Clustering_report\n",
    "\n",
    "extraS2_Clustering_report(cluster_save_path=os.getenv(\"Save_Clustering_Path\"),\n",
    "                              embeddings_faiss_path= os.getenv(\"Save_FAISS_DimenReduction_Path\"),\n",
    "                              cluster_centers_save_path=os.getenv(\"Save_Cluster_Centers_Path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: LLM-based labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import S3_EXCT_processor_main\n",
    "# Suppose you have a dictionary of all other EXCT_main parameters:\n",
    "exct_params = {\n",
    "    \"text_key\": \"abstract\", \n",
    "    \"Pydantic_Objects_List\": [],  # your pydantic models\n",
    "    \"path_to_list\": None,\n",
    "    \"model_engine\": \"OpenAI_Async\",\n",
    "    \"parser_error_handling\": \"llm_to_correct\",\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"pre_prompt\": \"\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"logprobs\": False,\n",
    "    \"seed\": None,\n",
    "    \"timeout\": 60,\n",
    "    \"max_retries\": 2,\n",
    "    \"openai_api_key\": os.getenv(\"OPENAI_COMPATIBLE_API_KEY\"),\n",
    "    \"runpod_base_url\": os.getenv(\"OPENAI_COMPATIBLE_BASE_URL\"),\n",
    "    \"runpod_api\": os.getenv(\"RUNPOD_API\"),\n",
    "    \"azure_api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    \"azure_api_version\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "    \"total_async_n\": 5,\n",
    "    # Note that we don't pass json_file_path or output_file_path here\n",
    "}\n",
    "\n",
    "folder_to_process = r\"C:\\path\\to\\folder\"\n",
    "filter_str = \"processed_\"  # e.g., only process JSON files that start with \"processed_\"\n",
    "prefix_str = \"extracted_\"\n",
    "\n",
    "S3_EXCT_processor_main(\n",
    "    folder_path=folder_to_process,\n",
    "    filter_startstring=filter_str,\n",
    "    add_string_at_beginning=prefix_str,\n",
    "    EXCT_main_kwargs_dictionary=exct_params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
